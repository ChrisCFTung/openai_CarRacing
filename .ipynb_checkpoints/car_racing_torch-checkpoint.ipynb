{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\chris\\anaconda3\\lib\\site-packages (0.11.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch==1.10.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torchvision) (1.10.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch==1.10.2->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: gym in c:\\users\\chris\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: gym[box2d] in c:\\users\\chris\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from gym[box2d]) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from gym[box2d]) (1.19.2)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from gym[box2d]) (1.5.21)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from gym[box2d]) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from gym[box2d]) (1.19.2)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'done'\n",
      "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp38-cp38-win_amd64.whl size=488216 sha256=ac9dd866f8e65a5cfed681ce2d8b6b7511a499c5798fa2ad513c48fde70b665b\n",
      "  Stored in directory: c:\\users\\chris\\appdata\\local\\pip\\cache\\wheels\\8b\\95\\16\\1dc99ff9a3f316ff245fdb5c9086cd13c35dad630809909075\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install gym\n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, random\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Environment*: CarRacing-v0\n",
    "\n",
    "*Action*: a combination of steering, stepping on gas, and stepping on brake (Can I mimmic a trail-braking?)\n",
    "\n",
    "*State*: the screen of the game\n",
    "\n",
    "*Reward*: using the default for now\n",
    "\n",
    "*Q*(s, a)*: will be approximated by a DNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1411..1768 -> 357-tiles track\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "while True:\n",
    "    #env.render()\n",
    "    state, reward, done, info = env.step([0,0,0])\n",
    "    #print(reward, done, info)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the action space with a wrapper\n",
    "class BasicWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.__actions__ = [[-0.7, 0.1, 0.],\n",
    "                             [0.7, 0.1, 0.],\n",
    "                             [0., 0.7, 0.],\n",
    "                             [0., 0., 0.7],\n",
    "                             [0., 0., 0.]]\n",
    "        \n",
    "        self.action_space = Box(0, 4, shape=(1,), dtype=np.int8)\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(self.__actions__[action])\n",
    "        # modify ...\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "# Skip some frames\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "#make the track into gray scale\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        h, w = self.observation_space.shape[:2]\n",
    "        obs_shape = (h-16, w)\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "    \n",
    "    def permute_orientation(self, observation):\n",
    "        observation = np.transpose(observation, (2,0,1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        observation = observation.squeeze(0)\n",
    "        return observation[:-16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_make():\n",
    "    env = gym.make('CarRacing-v0')\n",
    "    env = SkipFrame(env, 2)\n",
    "    env = BasicWrapper(env)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = FrameStack(env, num_stack=2)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 80, 96)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1151..1450 -> 299-tiles track\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(60):\n",
    "    env.step(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False {}\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(state[0])\n",
    "plt.show()\n",
    "print(done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LewisNet(nn.Module):\n",
    "    \"\"\"\n",
    "    cnn structure\n",
    "    input -> (conv2d + relu) X 3 -> flatten -> (dense + relu) X 2 -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "        \n",
    "        if h != 80:\n",
    "            raise ValueError(f\"Expecting input height: 96, got: {h}\")\n",
    "        if w != 96:\n",
    "            raise ValueError(f\"Expecting input height: 96, got: {w}\") \n",
    "            \n",
    "        # Create Q\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(384, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "        \n",
    "        # Create Q_target\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        # Freeze Q_target\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    def forward(self, inp, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(inp)\n",
    "        elif model == \"target\":\n",
    "            return self.target(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lewis:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        \"\"\"\n",
    "        An agent to drive the car\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.net = LewisNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "            \n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "        \n",
    "        self.save_every = 5e5 # no. of experiences between saving\n",
    "        \n",
    "        # cache and recall\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # learn\n",
    "        self.gamma = 0.9\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "        \n",
    "        self.burnin = 1e4\n",
    "        self.learn_every = 3\n",
    "        self.sync_every = 1e4\n",
    "        \n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state choose an epsilon-greedy action\n",
    "        \n",
    "        Inputs:\n",
    "        state(LazyFrame): a single observation, dimension = state_dim\n",
    "        Outputs:\n",
    "        action_idx(int): an integer representing the action in the action space\n",
    "        \"\"\"\n",
    "        # Explore\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "            \n",
    "        # Exploit\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model='online')\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "            \n",
    "        # Decrease exploration rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        \n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "    \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Add the experience to memory\n",
    "        Inputs:\n",
    "        state (LazyFrame)\n",
    "        next_state (LazyFrame)\n",
    "        action (int)\n",
    "        reward (float)\n",
    "        done (bool)\n",
    "        \n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "            \n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "            \n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Update the Q function with a batch of experience\"\"\"\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "            \n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "            \n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "        \n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "        \n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        \n",
    "        td_est = self.td_estimate(state, action)\n",
    "        \n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "        \n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "        \n",
    "        return (td_est.mean().item(), loss)\n",
    "    \n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]\n",
    "        return current_Q\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model='target')[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma*next_Q).float()\n",
    "    \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "        \n",
    "    def save(self):\n",
    "        save_path = (self.save_dir / f\"lewis_net_{int(self.curr_step//self.save_every)}.chkpt\")\n",
    "        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), save_path,)\n",
    "        print(f\"LewisNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x000002977A80EB80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 328, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 857, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x000002970680C130; to 'Win32Window' at 0x0000029706781C40>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "Track generation: 1179..1478 -> 299-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x000002977A80EB80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 328, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 857, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\Chris\\anaconda3\\lib\\_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x00000297071CF450; to 'Win32Window' at 0x000002970721F3A0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 500 - Epsilon 0.9998750077965335 - Mean Reward -29.53 - Mean Length 500.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 4.433 - Time 2022-02-02T20:32:13\n",
      "Track generation: 1109..1392 -> 283-tiles track\n",
      "Track generation: 1264..1595 -> 331-tiles track\n",
      "Track generation: 992..1250 -> 258-tiles track\n",
      "Track generation: 1023..1285 -> 262-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Track generation: 1117..1405 -> 288-tiles track\n",
      "Track generation: 1186..1493 -> 307-tiles track\n",
      "Track generation: 989..1241 -> 252-tiles track\n",
      "Track generation: 1044..1309 -> 265-tiles track\n",
      "Track generation: 1301..1629 -> 328-tiles track\n",
      "Track generation: 999..1253 -> 254-tiles track\n",
      "Track generation: 1158..1455 -> 297-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1203..1508 -> 305-tiles track\n",
      "Track generation: 1083..1358 -> 275-tiles track\n",
      "Track generation: 1205..1511 -> 306-tiles track\n",
      "Track generation: 1049..1315 -> 266-tiles track\n",
      "Track generation: 1181..1480 -> 299-tiles track\n",
      "Track generation: 1273..1596 -> 323-tiles track\n",
      "Track generation: 1167..1463 -> 296-tiles track\n",
      "Track generation: 1147..1445 -> 298-tiles track\n",
      "Track generation: 1189..1490 -> 301-tiles track\n",
      "Track generation: 1200..1504 -> 304-tiles track\n",
      "Episode 20 - Step 10500 - Epsilon 0.9973784419721954 - Mean Reward -27.131 - Mean Length 500.0 - Mean Loss 0.045 - Mean Q Value 0.055 - Time Delta 81.65 - Time 2022-02-02T20:33:35\n",
      "Track generation: 1008..1271 -> 263-tiles track\n",
      "Track generation: 1111..1402 -> 291-tiles track\n",
      "Track generation: 1025..1288 -> 263-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1260..1579 -> 319-tiles track\n",
      "Track generation: 1220..1529 -> 309-tiles track\n",
      "Track generation: 1077..1358 -> 281-tiles track\n",
      "Track generation: 1147..1438 -> 291-tiles track\n",
      "Track generation: 1074..1346 -> 272-tiles track\n",
      "Track generation: 1217..1528 -> 311-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1117..1403 -> 286-tiles track\n",
      "Track generation: 1126..1410 -> 284-tiles track\n",
      "Track generation: 1249..1565 -> 316-tiles track\n",
      "Track generation: 1344..1684 -> 340-tiles track\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1022..1287 -> 265-tiles track\n",
      "Track generation: 1050..1324 -> 274-tiles track\n",
      "Track generation: 1129..1415 -> 286-tiles track\n",
      "Track generation: 1016..1274 -> 258-tiles track\n",
      "Track generation: 923..1159 -> 236-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Track generation: 1144..1442 -> 298-tiles track\n",
      "Track generation: 1014..1271 -> 257-tiles track\n",
      "Track generation: 1191..1494 -> 303-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Episode 40 - Step 20500 - Epsilon 0.9948881097679284 - Mean Reward -28.296 - Mean Length 500.0 - Mean Loss 0.473 - Mean Q Value 0.421 - Time Delta 210.246 - Time 2022-02-02T20:37:05\n",
      "Track generation: 1039..1303 -> 264-tiles track\n",
      "Track generation: 1100..1379 -> 279-tiles track\n",
      "Track generation: 1123..1415 -> 292-tiles track\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Track generation: 1184..1484 -> 300-tiles track\n",
      "Track generation: 1412..1768 -> 356-tiles track\n",
      "Track generation: 1141..1430 -> 289-tiles track\n",
      "Track generation: 1141..1436 -> 295-tiles track\n",
      "Track generation: 1190..1501 -> 311-tiles track\n",
      "Track generation: 1106..1394 -> 288-tiles track\n",
      "Track generation: 1216..1524 -> 308-tiles track\n",
      "Track generation: 1288..1614 -> 326-tiles track\n",
      "Track generation: 1151..1446 -> 295-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1280..1613 -> 333-tiles track\n",
      "Track generation: 1323..1658 -> 335-tiles track\n",
      "Track generation: 1059..1332 -> 273-tiles track\n",
      "Track generation: 1008..1264 -> 256-tiles track\n",
      "Track generation: 1229..1541 -> 312-tiles track\n",
      "Track generation: 1014..1280 -> 266-tiles track\n",
      "Track generation: 1056..1324 -> 268-tiles track\n",
      "Episode 60 - Step 30500 - Epsilon 0.9924039956191436 - Mean Reward -27.458 - Mean Length 500.0 - Mean Loss 0.355 - Mean Q Value 0.251 - Time Delta 250.37 - Time 2022-02-02T20:41:15\n",
      "Track generation: 1249..1565 -> 316-tiles track\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Track generation: 935..1180 -> 245-tiles track\n",
      "Track generation: 1151..1443 -> 292-tiles track\n",
      "Track generation: 1044..1309 -> 265-tiles track\n",
      "Track generation: 1099..1378 -> 279-tiles track\n",
      "Track generation: 1254..1572 -> 318-tiles track\n",
      "Track generation: 1146..1436 -> 290-tiles track\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Track generation: 1061..1338 -> 277-tiles track\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Track generation: 1175..1473 -> 298-tiles track\n",
      "Track generation: 1336..1674 -> 338-tiles track\n",
      "Track generation: 1183..1490 -> 307-tiles track\n",
      "Track generation: 1271..1593 -> 322-tiles track\n",
      "Track generation: 1047..1318 -> 271-tiles track\n",
      "Track generation: 1173..1479 -> 306-tiles track\n",
      "Track generation: 1340..1679 -> 339-tiles track\n",
      "Track generation: 1176..1483 -> 307-tiles track\n",
      "Track generation: 1156..1449 -> 293-tiles track\n",
      "Episode 80 - Step 40500 - Epsilon 0.9899260840001056 - Mean Reward -28.003 - Mean Length 500.0 - Mean Loss 0.295 - Mean Q Value 0.12 - Time Delta 343.336 - Time 2022-02-02T20:46:58\n",
      "Track generation: 1061..1336 -> 275-tiles track\n",
      "Track generation: 1025..1287 -> 262-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1152..1452 -> 300-tiles track\n",
      "Track generation: 1118..1410 -> 292-tiles track\n",
      "Track generation: 1209..1522 -> 313-tiles track\n",
      "Track generation: 1201..1505 -> 304-tiles track\n",
      "Track generation: 1182..1482 -> 300-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1244..1559 -> 315-tiles track\n",
      "Track generation: 939..1181 -> 242-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 980..1228 -> 248-tiles track\n",
      "Track generation: 1162..1459 -> 297-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1344..1684 -> 340-tiles track\n",
      "Track generation: 1059..1328 -> 269-tiles track\n",
      "Track generation: 1124..1409 -> 285-tiles track\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Track generation: 1179..1478 -> 299-tiles track\n",
      "Track generation: 1195..1498 -> 303-tiles track\n",
      "Track generation: 1101..1380 -> 279-tiles track\n",
      "Track generation: 914..1153 -> 239-tiles track\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "lulu = lewis(state_dim=(2, 80, 96), action_dim=5, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 100\n",
    "env = env_make()\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = lulu.act(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        lulu.cache(state, next_state, action, reward, done)\n",
    "        q, loss = lulu.learn()\n",
    "        logger.log_step(reward, loss, q)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    logger.log_episode()\n",
    "    \n",
    "    if ep%20 ==0:\n",
    "        logger.record(episode=ep, epsilon=lulu.exploration_rate, step=lulu.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
